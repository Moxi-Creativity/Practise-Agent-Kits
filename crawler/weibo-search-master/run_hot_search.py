# encoding: utf-8
import requests
from lxml import etree
from datetime import datetime
import os
import sys
from scrapy.cmdline import execute
from scrapy.utils.project import get_project_settings
 
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

HOT_URL = "https://tophub.today/n/KqndgxeLl9"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.69',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Referer': 'https://tophub.today/'
}
MAX_HOT_KEYWORDS = 50
MAX_ITEMS_PER_KEYWORD = 1000
TODAY = datetime.now().strftime("%Y-%m-%d")


def safe_extract_text(xpath_result):
    try:
        return xpath_result[0].strip() if xpath_result and xpath_result[0].strip() else ""
    except (IndexError, AttributeError):
        return ""


def crawl_hot_keywords():
    hot_keywords = []
    print("=" * 80)
    print("ğŸ“Œ å¼€å§‹çˆ¬å–å¾®åšçƒ­æœå…³é”®è¯ï¼ˆè½¬æ¢ä¸ºè¯é¢˜æ ¼å¼ï¼‰")
    print("=" * 80)

    try:
        response = requests.get(url=HOT_URL, headers=HEADERS, timeout=15)
        response.encoding = "utf-8"
        html = etree.HTML(response.text)

        trs = html.xpath(
            '//div[contains(@class, "jc rank-all-item")]//div[@class="jc-c"]//table[@class="table"]//tbody/tr'
        )

        if not trs:
            print("âŒ æœªæ‰¾åˆ°çƒ­æœæ•°æ®ï¼Œè¯·æ£€æŸ¥XPathæˆ–æ•°æ®æºURLï¼")
            return hot_keywords

        print(f"âœ… æ‰¾åˆ° {len(trs)} æ¡çƒ­æœï¼ˆçˆ¬å–æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}ï¼‰")
        print("-" * 150)

        for i, tr in enumerate(trs):
            if i >= MAX_HOT_KEYWORDS:  # é™åˆ¶åªå–å‰Nä¸ªçƒ­æœ
                break

            # æå–çƒ­æœä¿¡æ¯
            rank = safe_extract_text(tr.xpath('./td[1]/text()')).replace('.', '')
            title = safe_extract_text(tr.xpath('./td[2]/a/text()'))
            hot_value = safe_extract_text(tr.xpath('./td[3]/text()'))

            # è¿‡æ»¤å¹¿å‘Šå’Œæ— æ•ˆæ ‡é¢˜
            if not title or "å¹¿å‘Š" in title:
                print(f"âŒ è·³è¿‡æ— æ•ˆçƒ­æœï¼ˆæ’å{rank}ï¼š{title}ï¼‰")
                continue

            # è½¬æ¢ä¸ºè¯é¢˜æ ¼å¼ï¼šé¦–å°¾æ·»åŠ #å·
            keyword = '#' + title.replace("#", "").strip() + '#'
            hot_keywords.append(keyword)

            print(f"æ’åï¼š{rank:2s} | æ ‡é¢˜ï¼š{title:<30} | çƒ­åº¦ï¼š{hot_value:8s} | è¯é¢˜å…³é”®è¯ï¼š{keyword}")

        print("-" * 150)
        print(f"âœ… æˆåŠŸæå– {len(hot_keywords)} ä¸ªæœ‰æ•ˆè¯é¢˜å…³é”®è¯")
        print("=" * 80)

    except requests.exceptions.RequestException as e:
        print(f"âŒ çƒ­æœçˆ¬å–å¤±è´¥ï¼ˆç½‘ç»œé”™è¯¯ï¼‰ï¼š{str(e)}")
    except Exception as e:
        print(f"âŒ çƒ­æœçˆ¬å–å¤±è´¥ï¼ˆå…¶ä»–é”™è¯¯ï¼‰ï¼š{str(e)}")

    return hot_keywords


def start_weibo_crawler(keywords):
    if not keywords:
        print("âŒ æ— æœ‰æ•ˆå…³é”®è¯ï¼Œçˆ¬è™«å¯åŠ¨å¤±è´¥ï¼")
        return

    print("\n" + "=" * 80)
    print(f"ğŸš€ å¯åŠ¨å¾®åšæœç´¢çˆ¬è™«ï¼ˆå•æ¬¡æµ‹è¯•æ¨¡å¼ï¼‰")
    print(f"ğŸ” æœç´¢å…³é”®è¯ï¼š{len(keywords)} ä¸ª")
    print(f"ğŸ“… æœç´¢æ—¶é—´ï¼š{TODAY}ï¼ˆå½“å¤©ï¼‰")
    print(f"ğŸ“Š é™åˆ¶æ¡æ•°ï¼šæ¯ä¸ªå…³é”®è¯å‰{MAX_ITEMS_PER_KEYWORD}æ¡")
    print("=" * 80)

    try:
        # å…³é”®ä¿®æ”¹ï¼šé€šè¿‡-aå‚æ•°ä¼ é€’å…³é”®è¯ç»™çˆ¬è™«
        cmd = [
            'scrapy', 'crawl', 'search',
            '-a', f'keywords={",".join(keywords)}',  # ç›´æ¥ä¼ é€’å…³é”®è¯åˆ—è¡¨
            '-s', f'START_DATE={TODAY}',
            '-s', f'END_DATE={TODAY}',
            '-s', f'MAX_ITEMS_PER_KEYWORD={MAX_ITEMS_PER_KEYWORD}',
            '-s', 'DOWNLOAD_DELAY=3',
            '-s', 'CONCURRENT_REQUESTS=1',
            '-s', 'LOG_LEVEL=INFO'
        ]
        execute(cmd)

    except Exception as e:
        print(f"âŒ çˆ¬è™«è¿è¡Œå¤±è´¥ï¼š{str(e)}")


if __name__ == "__main__":
    hot_keywords = crawl_hot_keywords()
    start_weibo_crawler(hot_keywords)
