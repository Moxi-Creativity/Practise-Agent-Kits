# -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html
# -*- coding: utf-8 -*-
import copy
import csv
import os
import re
import pymysql
from datetime import datetime
from urllib.parse import unquote

import scrapy
from scrapy.exceptions import DropItem
from scrapy.pipelines.files import FilesPipeline
from scrapy.pipelines.images import ImagesPipeline
from scrapy.utils.project import get_project_settings

settings = get_project_settings()


class CsvPipeline(object):
    def process_item(self, item, spider):
        # å¯¹å…³é”®è¯è¿›è¡ŒURLè§£ç ï¼Œè¿˜åŸä¸ºä¸­æ–‡è¯é¢˜åç§°
        keyword = unquote(item['keyword'])
        base_dir = f'ç»“æœæ–‡ä»¶{os.sep}{keyword}'
        if not os.path.isdir(base_dir):
            os.makedirs(base_dir)
        file_path = f'{base_dir}{os.sep}{keyword}.csv'
        if not os.path.isfile(file_path):
            is_first_write = 1
        else:
            is_first_write = 0

        if item:
            with open(file_path, 'a', encoding='utf-8-sig', newline='') as f:
                writer = csv.writer(f)
                if is_first_write:
                    header = [
                        'id', 'bid', 'user_id', 'ç”¨æˆ·æ˜µç§°', 'å¾®åšæ­£æ–‡', 'å¤´æ¡æ–‡ç« url',
                        'å‘å¸ƒä½ç½®', 'è‰¾ç‰¹ç”¨æˆ·', 'è¯é¢˜', 'è½¬å‘æ•°', 'è¯„è®ºæ•°', 'ç‚¹èµæ•°', 'å‘å¸ƒæ—¶é—´',
                        'å‘å¸ƒå·¥å…·', 'å¾®åšå›¾ç‰‡url', 'å¾®åšè§†é¢‘url', 'retweet_id', 'ip', 'user_authentication',
                        'ä¼šå‘˜ç±»å‹', 'ä¼šå‘˜ç­‰çº§'
                    ]
                    writer.writerow(header)

                writer.writerow([
                    item['weibo'].get('id', ''),
                    item['weibo'].get('bid', ''),
                    item['weibo'].get('user_id', ''),
                    item['weibo'].get('screen_name', ''),
                    item['weibo'].get('text', ''),
                    item['weibo'].get('article_url', ''),
                    item['weibo'].get('location', ''),
                    item['weibo'].get('at_users', ''),
                    item['weibo'].get('topics', ''),
                    item['weibo'].get('reposts_count', ''),
                    item['weibo'].get('comments_count', ''),
                    item['weibo'].get('attitudes_count', ''),
                    item['weibo'].get('created_at', ''),
                    item['weibo'].get('source', ''),
                    ','.join(item['weibo'].get('pics', [])),
                    item['weibo'].get('video_url', ''),
                    item['weibo'].get('retweet_id', ''),
                    item['weibo'].get('ip', ''),
                    item['weibo'].get('user_authentication', ''),
                    item['weibo'].get('vip_type', ''),
                    item['weibo'].get('vip_level', 0)
                ])
        return item


class SQLitePipeline(object):
    def open_spider(self, spider):
        try:
            import sqlite3
            base_dir = 'ç»“æœæ–‡ä»¶'
            if not os.path.isdir(base_dir):
                os.makedirs(base_dir)
            db_name = settings.get('SQLITE_DATABASE', 'weibo.db')
            self.conn = sqlite3.connect(os.path.join(base_dir, db_name))
            self.cursor = self.conn.cursor()
            sql = """
                  CREATE TABLE IF NOT EXISTS weibo
                  (
                      id
                      varchar
                  (
                      20
                  ) NOT NULL PRIMARY KEY,
                      bid varchar
                  (
                      12
                  ) NOT NULL,
                      user_id varchar
                  (
                      20
                  ),
                      screen_name varchar
                  (
                      30
                  ),
                      text varchar
                  (
                      2000
                  ),
                      article_url varchar
                  (
                      100
                  ),
                      topics varchar
                  (
                      200
                  ),
                      at_users varchar
                  (
                      1000
                  ),
                      pics varchar
                  (
                      3000
                  ),
                      video_url varchar
                  (
                      1000
                  ),
                      location varchar
                  (
                      100
                  ),
                      created_at DATETIME,
                      source varchar
                  (
                      30
                  ),
                      attitudes_count INTEGER,
                      comments_count INTEGER,
                      reposts_count INTEGER,
                      retweet_id varchar
                  (
                      20
                  ),
                      ip varchar
                  (
                      100
                  ),
                      user_authentication varchar
                  (
                      100
                  ),
                      vip_type varchar
                  (
                      50
                  ),
                      vip_level INTEGER
                      )"""
            self.cursor.execute(sql)
            self.conn.commit()
        except Exception as e:
            print(f"SQLiteæ•°æ®åº“åˆ›å»ºå¤±è´¥: {e}")
            spider.sqlite_error = True

    def process_item(self, item, spider):
        data = dict(item['weibo'])
        data['pics'] = ','.join(data['pics'])
        keys = ', '.join(data.keys())
        placeholders = ', '.join(['?'] * len(data))
        sql = f"""INSERT OR REPLACE INTO weibo ({keys}) 
                 VALUES ({placeholders})"""
        try:
            self.cursor.execute(sql, tuple(data.values()))
            self.conn.commit()
        except Exception as e:
            print(f"SQLiteä¿å­˜å‡ºé”™: {e}")
            spider.sqlite_error = True
            self.conn.rollback()

    def close_spider(self, spider):
        self.conn.close()


class MyImagesPipeline(ImagesPipeline):
    def get_media_requests(self, item, info):
        """é“¾æ¥æ‹¼æ¥é€»è¾‘ä¸å˜ï¼ˆä½ æ‰‹åŠ¨æµ‹è¯•æœ‰æ•ˆï¼Œä¿ç•™ï¼‰"""
        if item['weibo'].get('pics') and len(item['weibo']['pics']) > 0:
            for img_idx, img_url in enumerate(item['weibo']['pics']):
                if not img_url.strip():
                    continue
                # æ‹¼æ¥ç™¾åº¦ä¸‹è½½æ¥å£ï¼ˆä½ éªŒè¯è¿‡æœ‰æ•ˆï¼‰
                baidu_download_url = f"https://image.baidu.com/search/down?url={img_url}"
                print(f"ğŸ“¤ å‘èµ·å›¾ç‰‡ä¸‹è½½è¯·æ±‚ï¼š{baidu_download_url}")  # æ–°å¢æ—¥å¿—ï¼Œç¡®è®¤è¯·æ±‚URLæ­£ç¡®
                yield scrapy.Request(
                    url=baidu_download_url,
                    meta={
                        'item': item,
                        'img_idx': img_idx,
                        'original_img_url': img_url
                    },
                    dont_filter=True,
                    headers={
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',
                        'Referer': 'https://image.baidu.com/'
                    }
                )

    def file_path(self, request, response=None, info=None):
        """ã€å¼ºåˆ¶ä¿®æ­£ã€‘ä»…ä¿ç•™3ä¸ªå‚æ•°ï¼Œç»å¯¹ä¸æ·»åŠ itemï¼"""
        # æ–°å¢æ—¥å¿—ï¼šéªŒè¯å‚æ•°æ˜¯å¦æ­£ç¡®ï¼ˆæ— itemå‚æ•°ï¼‰
        print(f"ğŸ”§ file_path å‚æ•°ï¼šrequest={request}, response={response}, info={info}")

        # ä»request.metaæå–itemï¼ˆå”¯ä¸€æ­£ç¡®çš„æ–¹å¼ï¼‰
        item = request.meta.get('item')
        img_idx = request.meta.get('img_idx', 0)
        original_img_url = request.meta.get('original_img_url')

        # ç®€åŒ–è·¯å¾„é€»è¾‘ï¼Œå‡å°‘å‡ºé”™ç‚¹
        decoded_keyword = unquote(item['keyword'])
        clean_keyword = re.sub(r'[#@!$%^&*(){}[\];:"\'<>,.?\\/]', '_', decoded_keyword).strip('_')
        weibo_id = item['weibo']['id']

        # æå–åç¼€
        img_suffix = original_img_url.split('.')[-1] if '.' in original_img_url else 'jpg'
        img_suffix = img_suffix.split('?')[0]  # å½»åº•å»é™¤URLå‚æ•°
        img_suffix = '.' + img_suffix if len(img_suffix) <= 5 else '.jpg'

        # ç”Ÿæˆè·¯å¾„ï¼ˆç”¨os.path.joinç¡®ä¿è·¨ç³»ç»Ÿå…¼å®¹ï¼‰
        save_path = os.path.join(
            'ç»“æœæ–‡ä»¶',
            clean_keyword,
            'images',
            f'{weibo_id}_{img_idx}{img_suffix}'
        )
        print(f"ğŸ“ å›¾ç‰‡å­˜å‚¨è·¯å¾„ï¼š{save_path}")  # æ–°å¢æ—¥å¿—ï¼Œç¡®è®¤è·¯å¾„æ­£ç¡®
        return save_path

    def item_completed(self, results, item, info):
        success_count = sum(1 for ok, _ in results if ok)
        fail_count = len(results) - success_count
        decoded_keyword = unquote(item['keyword'])
        print(f"ğŸ“Š è¯é¢˜ã€Œ{decoded_keyword}ã€å›¾ç‰‡ä¸‹è½½ç»“æœï¼šæˆåŠŸ{success_count}å¼ ï¼Œå¤±è´¥{fail_count}å¼ ")
        return item


class MyVideoPipeline(FilesPipeline):
    def get_media_requests(self, item, info):
        if item['weibo']['video_url']:
            yield scrapy.Request(item['weibo']['video_url'],
                                 meta={'item': item})

    def file_path(self, request, response=None, info=None):
        item = request.meta['item']
        keyword = unquote(item['keyword'])  # è§£ç å…³é”®è¯ä¸ºä¸­æ–‡
        base_dir = f'ç»“æœæ–‡ä»¶{os.sep}{keyword}{os.sep}videos'
        if not os.path.isdir(base_dir):
            os.makedirs(base_dir)
        file_path = f'{base_dir}{os.sep}{item["weibo"]["id"]}.mp4'
        return file_path


class MongoPipeline(object):
    def open_spider(self, spider):
        try:
            from pymongo import MongoClient
            self.client = MongoClient(settings.get('MONGO_URI'))
            self.db = self.client['weibo']
            self.collection = self.db['weibo']
        except ModuleNotFoundError:
            spider.pymongo_error = True

    def process_item(self, item, spider):
        try:
            import pymongo
            new_item = copy.deepcopy(item)
            if not self.collection.find_one({'id': new_item['weibo']['id']}):
                self.collection.insert_one(dict(new_item['weibo']))
            else:
                self.collection.update_one({'id': new_item['weibo']['id']},
                                           {'$set': dict(new_item['weibo'])})
        except pymongo.errors.ServerSelectionTimeoutError:
            spider.mongo_error = True

    def close_spider(self, spider):
        try:
            self.client.close()
        except AttributeError:
            pass


class MysqlPipeline(object):
    def __init__(self):
        self.db = None
        self.cursor = None
        self.today_db = datetime.now().strftime('weibo_%Y_%m_%d')  # å½“å¤©æ—¥æœŸæ•°æ®åº“å

    def clean_topic_name(self, topic):
        """æ¸…æ´—è¯é¢˜åç§°ä¸ºåˆæ³•MySQLè¡¨å"""
        clean_name = re.sub(r'[#@!$%^&*(){}[\];:"\'<>,.?\\/]', '_', topic).strip('_')
        clean_name = clean_name[:50] if clean_name else 'default_topic'
        return clean_name

    def create_date_database(self, mysql_config):
        """åˆ›å»ºå½“å¤©çš„æ—¥æœŸæ•°æ®åº“ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰"""
        try:
            server_config = mysql_config.copy()
            server_config.pop('db', None)  # ä¸æŒ‡å®šå…·ä½“æ•°æ®åº“ï¼Œä»…è¿æ¥æœåŠ¡å™¨
            db_server = pymysql.connect(**server_config)
            cursor_server = db_server.cursor()

            sql = f"""CREATE DATABASE IF NOT EXISTS `{self.today_db}` 
                      DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci"""
            cursor_server.execute(sql)
            print(f"âœ… æ—¥æœŸæ•°æ®åº“`{self.today_db}`åˆ›å»ºæˆåŠŸï¼ˆæˆ–å·²å­˜åœ¨ï¼‰")

            cursor_server.close()
            db_server.close()
        except Exception as e:
            print(f"âŒ åˆ›å»ºæ—¥æœŸæ•°æ®åº“å¤±è´¥ï¼š{e}")
            raise

    def create_topic_table(self, table_name):
        """åˆ›å»ºå•ä¸ªè¯é¢˜çš„è¡¨ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰"""
        sql = f"""
            CREATE TABLE IF NOT EXISTS `{table_name}` (
                id varchar(20) NOT NULL,
                bid varchar(12) NOT NULL,
                user_id varchar(20),
                screen_name varchar(30),
                text TEXT NOT NULL,
                article_url varchar(100),
                topics varchar(500),
                at_users varchar(1000),
                pics varchar(3000),
                video_url varchar(1000),
                location varchar(100),
                created_at DATETIME,
                source varchar(30),
                attitudes_count INT,
                comments_count INT,
                reposts_count INT,
                retweet_id varchar(20),
                ip varchar(100),
                user_authentication varchar(100),
                vip_type varchar(50),
                vip_level INT,
                PRIMARY KEY (id)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
        """
        self.cursor.execute(sql)
        print(f"âœ… è¯é¢˜è¡¨`{table_name}`åˆ›å»ºæˆåŠŸï¼ˆæˆ–å·²å­˜åœ¨ï¼‰")

    def open_spider(self, spider):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ï¼Œåˆ›å»ºæ—¥æœŸæ•°æ®åº“"""
        try:
            from scrapy.utils.project import get_project_settings
            settings = get_project_settings()
            mysql_config = {
                'host': settings.get('MYSQL_HOST', 'localhost'),
                'port': settings.get('MYSQL_PORT', 3306),
                'user': settings.get('MYSQL_USER', 'root'),
                'password': settings.get('MYSQL_PASSWORD', '123456'),
                'charset': 'utf8mb4',
                'connect_timeout': 10
            }

            # 1. åˆ›å»ºå½“å¤©çš„æ—¥æœŸæ•°æ®åº“
            self.create_date_database(mysql_config)

            # 2. è¿æ¥æ—¥æœŸæ•°æ®åº“
            mysql_config['db'] = self.today_db
            self.db = pymysql.connect(**mysql_config)
            self.cursor = self.db.cursor()
            print(f"âœ… MySQLæ—¥æœŸæ•°æ®åº“`{self.today_db}`è¿æ¥æˆåŠŸ")

        except ImportError:
            spider.pymysql_error = True
            print("âŒ æœªå®‰è£…pymysqlï¼Œè¯·æ‰§è¡Œï¼špip install pymysql")
        except pymysql.OperationalError as e:
            spider.mysql_error = True
            print(f"âŒ MySQLè¿æ¥å¤±è´¥ï¼š{e}")

    def process_item(self, item, spider):
        if not self.db or not self.cursor:
            return item

        # 1. æ¸…æ´—å¹¶ç”Ÿæˆè¯é¢˜è¡¨å
        raw_topic = unquote(item['keyword'])
        clean_topic = self.clean_topic_name(raw_topic)
        topic_table_name = clean_topic

        # 2. ç¡®ä¿è¯é¢˜è¡¨å­˜åœ¨
        self.create_topic_table(topic_table_name)

        # 3. æ’å…¥/æ›´æ–°æ•°æ®
        data = dict(item['weibo'])
        data['pics'] = ','.join(data['pics'])
        keys = ', '.join([f'`{k}`' for k in data.keys()])
        values = ', '.join(['%s'] * len(data))

        sql = f"""
            INSERT INTO `{topic_table_name}` ({keys}) 
            VALUES ({values}) 
            ON DUPLICATE KEY UPDATE 
            {', '.join([f'`{k}` = %s' for k in data.keys()])}
        """
        try:
            self.cursor.execute(sql, tuple(data.values()) * 2)
            self.db.commit()
            print(f"âœ… æˆåŠŸå­˜å‚¨å¾®åšï¼ˆåº“ï¼š{self.today_db}ï¼Œè¡¨ï¼š{topic_table_name}ï¼ŒIDï¼š{data['id']}ï¼‰")
        except Exception as e:
            self.db.rollback()
            print(f"âŒ æ•°æ®å­˜å‚¨å¤±è´¥ï¼ˆåº“ï¼š{self.today_db}ï¼Œè¡¨ï¼š{topic_table_name}ï¼ŒIDï¼š{data.get('id', 'æœªçŸ¥')}ï¼‰ï¼š{e}")
        return item

    def close_spider(self, spider):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.db:
            self.db.close()
            print(f"âœ… MySQLæ—¥æœŸæ•°æ®åº“`{self.today_db}`è¿æ¥å·²å…³é—­")


class DuplicatesPipeline(object):
    def __init__(self):
        self.ids_seen = set()

    def process_item(self, item, spider):
        if item['weibo']['id'] in self.ids_seen:
            raise DropItem("è¿‡æ»¤é‡å¤å¾®åš: %s" % item)
        else:
            self.ids_seen.add(item['weibo']['id'])
            return item
